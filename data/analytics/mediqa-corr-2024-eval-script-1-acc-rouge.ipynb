{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## **Evaluation Script for MEDIQA-CORR 2024**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "gather": {
          "logged": 1706307625356
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "gather": {
          "logged": 1706759047762
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from rouge import Rouge\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "import math\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "gather": {
          "logged": 1706759049626
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#################\n",
        "# Parsing Funcs #\n",
        "#################\n",
        "\n",
        "\n",
        "def parse_reference_file(filepath):\n",
        "    \"\"\"Parsing reference file path.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, encoding='utf-8')\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(filepath, encoding='ISO-8859-1')\n",
        "    \n",
        "    reference_corrections = {}\n",
        "    reference_flags = {}\n",
        "    reference_sent_id = {}\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        text_id = row['Text ID']\n",
        "        corrected_sentence = row['Corrected Sentence']\n",
        "        \n",
        "        if not isinstance(corrected_sentence, str):\n",
        "            if math.isnan(corrected_sentence):\n",
        "                corrected_sentence = \"NA\"\n",
        "            else:\n",
        "                corrected_sentence = str(corrected_sentence)\n",
        "                corrected_sentence = corrected_sentence.replace(\"\\n\", \" \") \\\n",
        "                  .replace(\"\\r\", \" \").strip()\n",
        "                  \n",
        "        reference_corrections[text_id] = corrected_sentence\n",
        "        reference_flags[text_id] = str(row['Error Flag'])\n",
        "        reference_sent_id[text_id] = str(row['Error Sentence ID'])\n",
        "\n",
        "    return reference_corrections, reference_flags, reference_sent_id\n",
        "\n",
        "\n",
        "def parse_run_submission_file(filepath):\n",
        "    \"\"\"Parsing submission file path.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            lines = file.readlines()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(filepath, 'r', encoding='ISO-8859-1') as file:\n",
        "            lines = file.readlines()\n",
        "    \n",
        "    candidate_corrections = {}\n",
        "    predicted_flags = {}\n",
        "    candidate_sent_id = {}\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "            \n",
        "        if not re.fullmatch('[a-z0-9\\-]+\\s[0-9]+\\s\\-?[0-9]+\\s.+', line):\n",
        "            print(\"Invalid line: \", line)\n",
        "            continue\n",
        "            \n",
        "        # Replacing consecutive spaces with a single space\n",
        "        line = re.sub('\\s+', ' ', line)\n",
        "        \n",
        "        # Parsing\n",
        "        items = line.split()\n",
        "        text_id = items[0]\n",
        "        error_flag = items[1]\n",
        "        sentence_id = items[2]\n",
        "        corrected_sentence = ' '.join(items[3:]).strip()\n",
        "        \n",
        "        predicted_flags[text_id] = error_flag\n",
        "        candidate_sent_id[text_id] = sentence_id\n",
        "\n",
        "        if error_flag == '0':\n",
        "            candidate_corrections[text_id] = \"NA\"\n",
        "        else:\n",
        "            candidate_corrections[text_id] = corrected_sentence.strip('\"')\n",
        "\n",
        "    return candidate_corrections, predicted_flags, candidate_sent_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "gather": {
          "logged": 1706759187649
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "##############\n",
        "# Eval Funcs #\n",
        "##############\n",
        "\n",
        "def compute_accuracy(reference_flags, reference_sent_id, predicted_flags, candidate_sent_id):\n",
        "    # Error Flags Accuracy (missing predictions are counted as false)\n",
        "    matching_flags_nb = 0\n",
        "    \n",
        "    for text_id in reference_flags:\n",
        "        if text_id in predicted_flags and reference_flags[text_id] == predicted_flags[text_id]:\n",
        "            matching_flags_nb += 1\n",
        "            \n",
        "    flags_accuracy = matching_flags_nb / len(reference_flags)\n",
        "    \n",
        "    # Error Sentence Detection Accuracy (missing predictions are counted as false)\n",
        "    matching_sentence_nb = 0\n",
        "    \n",
        "    for text_id in reference_sent_id:\n",
        "        if text_id in candidate_sent_id and candidate_sent_id[text_id] == reference_sent_id[text_id]:\n",
        "            matching_sentence_nb += 1\n",
        "            \n",
        "    sent_accuracy = matching_sentence_nb / len(reference_sent_id)\n",
        "\n",
        "    return {\n",
        "        \"Error Flags Accuracy\": flags_accuracy,\n",
        "        \"Error Sentence Detection Accuracy\": sent_accuracy\n",
        "    }\n",
        "\n",
        "def increment_counter(counters, counter_name):\n",
        "    counters[counter_name] = counters[counter_name] + 1\n",
        "\n",
        "class NLGMetrics(object):\n",
        "\n",
        "    def __init__(self, metrics = ['ROUGE']):\n",
        "        self.metrics = metrics\n",
        "    \n",
        "    def compute(self, references, predictions, counters):\n",
        "        results = {}\n",
        "        \n",
        "        if 'ROUGE' in self.metrics:\n",
        "            rouge = Rouge() \n",
        "            rouge_scores = rouge.get_scores(predictions, references)\n",
        "                            \n",
        "            rouge1f_scores = []\n",
        "            rouge2f_scores = []\n",
        "            rougeLf_scores = []\n",
        "            \n",
        "            for i in range(len(references)):\n",
        "                r1f = rouge_scores[i][\"rouge-1\"][\"f\"]\n",
        "                r2f = rouge_scores[i][\"rouge-2\"][\"f\"]\n",
        "                rlf = rouge_scores[i][\"rouge-l\"][\"f\"]\n",
        "                \n",
        "                rouge1f_scores.append(r1f)\t\n",
        "                rouge2f_scores.append(r2f)\n",
        "                rougeLf_scores.append(rlf)\n",
        "                \n",
        "            # for checking comparison with composite\n",
        "            rouge1check = np.array(rouge1f_scores).mean()\n",
        "            rouge2check = np.array(rouge2f_scores).mean()\n",
        "            rougeLcheck = np.array(rougeLf_scores).mean()\n",
        "\n",
        "            results['R1F_subset_check'] = rouge1check\n",
        "            results['R2F_subset_check'] = rouge2check\n",
        "            results['RLF_subset_check'] = rougeLcheck\n",
        "            \n",
        "            ###############################\n",
        "            # Composite score computation #\n",
        "            ###############################\n",
        "            \n",
        "            \"\"\"\n",
        "            NLG METRIC on sentence vs. sentence cases + ones or zeros \n",
        "            when either the reference or the candidate correction is NA\n",
        "            \"\"\"\n",
        "            \n",
        "            rouge1score = np.array(rouge1f_scores).sum()\n",
        "            rouge2score = np.array(rouge2f_scores).sum()\n",
        "            rougeLscore = np.array(rougeLf_scores).sum()\n",
        "            \n",
        "            composite_score_rouge1 = (rouge1score + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "            composite_score_rouge2 = (rouge2score + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "            composite_score_rougeL = (rougeLscore + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "\n",
        "            results['R1FC'] = composite_score_rouge1\n",
        "            results['R2FC'] = composite_score_rouge2\n",
        "            results['RLFC'] = composite_score_rougeL\n",
        "\n",
        "        \n",
        "        return results\n",
        "\n",
        "\n",
        "def get_nlg_eval_data(reference_corrections, candidate_corrections, remove_nonprint = False):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    \n",
        "    counters = {\n",
        "        \"total_texts\": 0,\n",
        "        \"reference_na\": 0,\n",
        "        \"total_system_texts\": 0,\n",
        "        \"system_provided_na\": 0,\n",
        "        \"system_provided_correct_na\": 0,\n",
        "    }\n",
        "    \n",
        "    for text_id in reference_corrections:\n",
        "        increment_counter(counters, \"total_texts\")\n",
        "        \n",
        "        # removing non ascii chars\n",
        "        reference_correction = reference_corrections[text_id]\n",
        "        \n",
        "        if remove_nonprint:\n",
        "            reference_correction = ''.join(filter(lambda x: x in string.printable, str(reference_correction)))\n",
        "            \n",
        "        if reference_correction == \"NA\":\n",
        "            increment_counter(counters, \"reference_na\")\n",
        "            \n",
        "        if text_id in candidate_corrections:\n",
        "            increment_counter(counters, \"total_system_texts\")\n",
        "            candidate = candidate_corrections[text_id]\n",
        "            \n",
        "            if remove_nonprint:\n",
        "                candidate = ''.join(filter(lambda x: x in string.printable, candidate))\n",
        "                \n",
        "            if candidate == \"NA\":\n",
        "                increment_counter(counters, \"system_provided_na\")\n",
        "                \n",
        "            # matching NA counts as 1\n",
        "            if reference_correction == \"NA\" and candidate == \"NA\":\n",
        "                increment_counter(counters, \"system_provided_correct_na\")\n",
        "                continue\n",
        "                \n",
        "            # Run provided \"NA\" when a correction was required (=> 0)\n",
        "            # or Run provided a correction when \"NA\" was required (=> 0)\n",
        "            if candidate == \"NA\" or reference_correction == \"NA\":\n",
        "                continue\n",
        "                \n",
        "            # remaining case is both reference and candidate are not \"NA\"\n",
        "            # both are inserted/added for ROUGE/BLEURT/etc. computation\n",
        "            references.append(reference_correction)\n",
        "            predictions.append(candidate)\n",
        "    \n",
        "    return references, predictions, counters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "# submission_file_path = \"C:/Users/USER/Documents/Maestria/CAOBA/Medical_Error/data/baseline-run-1.txt\"\n",
        "\n",
        "# with open(submission_file_path, 'r', encoding='utf-8') as file:\n",
        "#     content = file.read()\n",
        "#     print(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reference_csv_file = pd.read_csv('C:/Users/USER/Documents/Maestria/CAOBA/Medical_Error/data/MEDIQA-CORR-2024-MS-ValidationSet-1.csv')\n",
        "# reference_csv_file.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La columna 'Corrected Sentence' existe.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "reference_csv_file = 'C:/Users/USER/Documents/Maestria/CAOBA/Medical_Error/data/MEDIQA-CORR-2024-MS-ValidationSet-1.csv'\n",
        "\n",
        "# Aseg√∫rate de que el archivo se lea correctamente\n",
        "df = pd.read_csv(reference_csv_file)\n",
        "# Verificar que la columna 'Corrected Sentence' exista\n",
        "if 'Corrected Sentence' in df.columns:\n",
        "    print(\"La columna 'Corrected Sentence' existe.\")\n",
        "else:\n",
        "    print(\"La columna 'Corrected Sentence' no existe. Verifica el nombre y la estructura del archivo.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "submission_file = \"C:/Users/USER/Documents/Maestria/CAOBA/Medical_Error/data/prediction_Gru.txt\"\n",
        "\n",
        "try:\n",
        "    with open(submission_file, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "except UnicodeDecodeError:\n",
        "    try:\n",
        "        with open(submission_file, 'r', encoding='ISO-8859-1') as file:\n",
        "            content = file.read()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(submission_file, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            content = file.read()\n",
        "\n",
        "reference_csv_file = 'C:/Users/USER/Documents/Maestria/CAOBA/Medical_Error/data/MEDIQA-CORR-2024-MS-ValidationSet-1.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(reference_csv_file, encoding='utf-8')\n",
        "except UnicodeDecodeError:\n",
        "    try:\n",
        "        df = pd.read_csv(reference_csv_file, encoding='ISO-8859-1')\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(reference_csv_file, encoding='utf-8', errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [],
      "source": [
        "reference_corrections, reference_flags, reference_sent_id = parse_reference_file(reference_csv_file)\n",
        "candidate_corrections, candidate_flags, candidate_sent_id = parse_run_submission_file(submission_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "gather": {
          "logged": 1706759191583
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Results:\n",
            " {'Error Flags Accuracy': 0.5557491289198606, 'Error Sentence Detection Accuracy': 1.0}\n",
            "\n",
            "NLG Eval Results:\n",
            " {'R1F_subset_check': 0.9968890454063583, 'R2F_subset_check': 0.9959542549865581, 'RLF_subset_check': 0.9968890454063583, 'R1FC': 0.5540527373034672, 'R2FC': 0.5535429996703706, 'RLFC': 0.5540527373034672}\n",
            "\n",
            "{'total_texts': 574, 'reference_na': 255, 'total_system_texts': 574, 'system_provided_na': 12, 'system_provided_correct_na': 6}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Accuracy\n",
        "accuracy_results = compute_accuracy(reference_flags, reference_sent_id, candidate_flags, candidate_sent_id)\n",
        "print(\"Accuracy Results:\\n\", accuracy_results)\n",
        "print()\n",
        "\n",
        "# NLG Eval for corrections\n",
        "references, predictions, counters = get_nlg_eval_data(reference_corrections, candidate_corrections)\n",
        "metrics = NLGMetrics()\n",
        "nlg_eval_results = metrics.compute(references, predictions, counters) \n",
        "\n",
        "\n",
        "print(\"NLG Eval Results:\\n\", nlg_eval_results) \n",
        "print()\n",
        "\n",
        "# debug check\n",
        "print(counters)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
