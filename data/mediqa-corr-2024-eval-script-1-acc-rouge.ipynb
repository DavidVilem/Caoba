{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## **Evaluation Script for MEDIQA-CORR 2024**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1706307625356
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1706759047762
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from rouge import Rouge\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "import math\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1706759049626
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#################\n",
        "# Parsing Funcs #\n",
        "#################\n",
        "\n",
        "\n",
        "def parse_reference_file(filepath):\n",
        "    \"\"\"Parsing reference file path.\n",
        "\n",
        "    Returns:\n",
        "        reference_corrections (dict) {text_id: \"reference correction\"}\n",
        "        reference_flags (dict) {text_id: \"1 or 0 error flag\"}\n",
        "        reference_sent_id (dict) {text_id: \"error sentence id or -1\"}\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    reference_corrections = {}\n",
        "    reference_flags = {}\n",
        "    reference_sent_id = {}\n",
        "\n",
        "    df = pd.read_csv(filepath)\n",
        "    \n",
        "    for index, row in df.iterrows():\n",
        "        text_id = row['Text ID']\n",
        "        corrected_sentence = row['Corrected Sentence']\n",
        "        \n",
        "        if not isinstance(corrected_sentence, str):\n",
        "            if math.isnan(corrected_sentence):\n",
        "                corrected_sentence = \"NA\"\n",
        "            else:\n",
        "                corrected_sentence = str(corrected_sentence)\n",
        "                corrected_sentence = corrected_sentence.replace(\"\\n\", \" \") \\\n",
        "                  .replace(\"\\r\", \" \").strip()\n",
        "                  \n",
        "        reference_corrections[text_id] = corrected_sentence\n",
        "        reference_flags[text_id] = str(row['Error Flag'])\n",
        "        reference_sent_id[text_id] = str(row['Error Sentence ID'])\n",
        "\n",
        "    return reference_corrections, reference_flags, reference_sent_id\n",
        "\n",
        "\n",
        "def parse_run_submission_file(filepath):\n",
        "    \n",
        "    file = open(filepath, 'r')\n",
        "\n",
        "    candidate_corrections = {}\n",
        "    predicted_flags = {}\n",
        "    candidate_sent_id = {}\n",
        "    \n",
        "    lines = file.readlines()\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "            \n",
        "        if not re.fullmatch('[a-z0-9\\-]+\\s[0-9]+\\s\\-?[0-9]+\\s.+', line):\n",
        "            print(\"Invalid line: \", line)\n",
        "            continue\n",
        "            \n",
        "        # replacing consecutive spaces\n",
        "        line = re.sub('\\s+', line, ' ')\n",
        "        \n",
        "        # parsing\n",
        "        items = line.split()\n",
        "        text_id = items[0]\n",
        "        error_flag = items[1]\n",
        "        sentence_id = items[2]\n",
        "        corrected_sentence = ' '.join(items[3:]).strip()\n",
        "        \n",
        "        # debug - parsing check\n",
        "        # print(\"{} -- {} -- {} -- {}\".format(text_id, error_flag, sentence_id, corrected_sentence))\n",
        "\n",
        "        predicted_flags[text_id] = error_flag\n",
        "        candidate_sent_id[text_id] = sentence_id\n",
        "\n",
        "        # processing candidate corrections\n",
        "        # removing quotes\n",
        "\n",
        "        while corrected_sentence.startswith('\"') and len(corrected_sentence) > 1:\n",
        "            corrected_sentence = corrected_sentence[1:]\n",
        "            \n",
        "        while corrected_sentence.endswith('\"') and len(corrected_sentence) > 1:\n",
        "            corrected_sentence = corrected_sentence[:-1]\n",
        "                   \n",
        "        if error_flag == '0':\n",
        "            # enforcing \"NA\" in predicted non-errors (used for consistent/reliable eval)\n",
        "            candidate_corrections[text_id] = \"NA\"\n",
        "        else:\n",
        "            candidate_corrections[text_id] = corrected_sentence\n",
        "\n",
        "    return candidate_corrections, predicted_flags, candidate_sent_id\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1706759187649
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "##############\n",
        "# Eval Funcs #\n",
        "##############\n",
        "\n",
        "def compute_accuracy(reference_flags, reference_sent_id, predicted_flags, candidate_sent_id):\n",
        "    # Error Flags Accuracy (missing predictions are counted as false)\n",
        "    matching_flags_nb = 0\n",
        "    \n",
        "    for text_id in reference_flags:\n",
        "        if text_id in predicted_flags and reference_flags[text_id] == predicted_flags[text_id]:\n",
        "            matching_flags_nb += 1\n",
        "            \n",
        "    flags_accuracy = matching_flags_nb / len(reference_flags)\n",
        "    \n",
        "    # Error Sentence Detection Accuracy (missing predictions are counted as false)\n",
        "    matching_sentence_nb = 0\n",
        "    \n",
        "    for text_id in reference_sent_id:\n",
        "        if text_id in candidate_sent_id and candidate_sent_id[text_id] == reference_sent_id[text_id]:\n",
        "            matching_sentence_nb += 1\n",
        "            \n",
        "    sent_accuracy = matching_sentence_nb / len(reference_sent_id)\n",
        "\n",
        "    return {\n",
        "        \"Error Flags Accuracy\": flags_accuracy,\n",
        "        \"Error Sentence Detection Accuracy\": sent_accuracy\n",
        "    }\n",
        "\n",
        "def increment_counter(counters, counter_name):\n",
        "    counters[counter_name] = counters[counter_name] + 1\n",
        "\n",
        "class NLGMetrics(object):\n",
        "\n",
        "    def __init__(self, metrics = ['ROUGE']):\n",
        "        self.metrics = metrics\n",
        "    \n",
        "    def compute(self, references, predictions, counters):\n",
        "        results = {}\n",
        "        \n",
        "        if 'ROUGE' in self.metrics:\n",
        "            rouge = Rouge() \n",
        "            rouge_scores = rouge.get_scores(predictions, references)\n",
        "                            \n",
        "            rouge1f_scores = []\n",
        "            rouge2f_scores = []\n",
        "            rougeLf_scores = []\n",
        "            \n",
        "            for i in range(len(references)):\n",
        "                r1f = rouge_scores[i][\"rouge-1\"][\"f\"]\n",
        "                r2f = rouge_scores[i][\"rouge-2\"][\"f\"]\n",
        "                rlf = rouge_scores[i][\"rouge-l\"][\"f\"]\n",
        "                \n",
        "                rouge1f_scores.append(r1f)\t\n",
        "                rouge2f_scores.append(r2f)\n",
        "                rougeLf_scores.append(rlf)\n",
        "                \n",
        "            # for checking comparison with composite\n",
        "            rouge1check = np.array(rouge1f_scores).mean()\n",
        "            rouge2check = np.array(rouge2f_scores).mean()\n",
        "            rougeLcheck = np.array(rougeLf_scores).mean()\n",
        "\n",
        "            results['R1F_subset_check'] = rouge1check\n",
        "            results['R2F_subset_check'] = rouge2check\n",
        "            results['RLF_subset_check'] = rougeLcheck\n",
        "            \n",
        "            ###############################\n",
        "            # Composite score computation #\n",
        "            ###############################\n",
        "            \n",
        "            \"\"\"\n",
        "            NLG METRIC on sentence vs. sentence cases + ones or zeros \n",
        "            when either the reference or the candidate correction is NA\n",
        "            \"\"\"\n",
        "            \n",
        "            rouge1score = np.array(rouge1f_scores).sum()\n",
        "            rouge2score = np.array(rouge2f_scores).sum()\n",
        "            rougeLscore = np.array(rougeLf_scores).sum()\n",
        "            \n",
        "            composite_score_rouge1 = (rouge1score + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "            composite_score_rouge2 = (rouge2score + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "            composite_score_rougeL = (rougeLscore + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
        "\n",
        "            results['R1FC'] = composite_score_rouge1\n",
        "            results['R2FC'] = composite_score_rouge2\n",
        "            results['RLFC'] = composite_score_rougeL\n",
        "\n",
        "        \n",
        "        return results\n",
        "\n",
        "\n",
        "def get_nlg_eval_data(reference_corrections, candidate_corrections, remove_nonprint = False):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    \n",
        "    counters = {\n",
        "        \"total_texts\": 0,\n",
        "        \"reference_na\": 0,\n",
        "        \"total_system_texts\": 0,\n",
        "        \"system_provided_na\": 0,\n",
        "        \"system_provided_correct_na\": 0,\n",
        "    }\n",
        "    \n",
        "    for text_id in reference_corrections:\n",
        "        increment_counter(counters, \"total_texts\")\n",
        "        \n",
        "        # removing non ascii chars\n",
        "        reference_correction = reference_corrections[text_id]\n",
        "        \n",
        "        if remove_nonprint:\n",
        "            reference_correction = ''.join(filter(lambda x: x in string.printable, str(reference_correction)))\n",
        "            \n",
        "        if reference_correction == \"NA\":\n",
        "            increment_counter(counters, \"reference_na\")\n",
        "            \n",
        "        if text_id in candidate_corrections:\n",
        "            increment_counter(counters, \"total_system_texts\")\n",
        "            candidate = candidate_corrections[text_id]\n",
        "            \n",
        "            if remove_nonprint:\n",
        "                candidate = ''.join(filter(lambda x: x in string.printable, candidate))\n",
        "                \n",
        "            if candidate == \"NA\":\n",
        "                increment_counter(counters, \"system_provided_na\")\n",
        "                \n",
        "            # matching NA counts as 1\n",
        "            if reference_correction == \"NA\" and candidate == \"NA\":\n",
        "                increment_counter(counters, \"system_provided_correct_na\")\n",
        "                continue\n",
        "                \n",
        "            # Run provided \"NA\" when a correction was required (=> 0)\n",
        "            # or Run provided a correction when \"NA\" was required (=> 0)\n",
        "            if candidate == \"NA\" or reference_correction == \"NA\":\n",
        "                continue\n",
        "                \n",
        "            # remaining case is both reference and candidate are not \"NA\"\n",
        "            # both are inserted/added for ROUGE/BLEURT/etc. computation\n",
        "            references.append(reference_correction)\n",
        "            predictions.append(candidate)\n",
        "    \n",
        "\n",
        "    \n",
        "    return references, predictions, counters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# submission_file = \"C:/Users/USER/Documents/Maestria/CAOBA/Medical_Error/data/baseline-run-1.txt\"\n",
        "# reference_csv_file = \"C:/Users/USER/Documents/Maestria/CAOBA/Medical_Error/data/prediction.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\USER\\\\Documents\\\\Maestria\\\\CAOBA\\\\Medical_Error\\\\data\\\\'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sep = os.path.sep\n",
        "dir_actual = os.path.abspath('')\n",
        "PATH = sep.join(dir_actual.split(sep)[:-1])\n",
        "DIR_DATA = PATH + '{0}data{0}'.format(os.sep)\n",
        "sys.path.append(DIR_DATA) if DIR_DATA not in list(sys.path) else None\n",
        "DIR_DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_file = pd.read_csv(DIR_DATA + 'prediction.txt')\n",
        "reference_csv_file = pd.read_csv(DIR_DATA + 'MEDIQA-CORR-2024-MS-ValidationSet-1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "print(type(submission_file))\n",
        "print(type(reference_csv_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "gather": {
          "logged": 1706759191583
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Invalid file path or buffer object type: <class 'pandas.core.frame.DataFrame'>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-63-2805dbebc7b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreference_corrections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference_flags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference_sent_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_reference_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreference_csv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcandidate_corrections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_flags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_sent_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_run_submission_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0maccuracy_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreference_flags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference_sent_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_flags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_sent_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-24-6053d4623566>\u001b[0m in \u001b[0;36mparse_reference_file\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mreference_sent_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;31m# though mypy handling of conditional imports is difficult.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0m\u001b[0;32m    435\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m     )\n",
            "\u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\PythonCodingPack\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Invalid file path or buffer object type: {type(filepath_or_buffer)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'pandas.core.frame.DataFrame'>"
          ]
        }
      ],
      "source": [
        "reference_corrections, reference_flags, reference_sent_id = parse_reference_file(reference_csv_file)\n",
        "candidate_corrections, candidate_flags, candidate_sent_id = parse_run_submission_file(submission_file)\n",
        "\n",
        "# Accuracy\n",
        "accuracy_results = compute_accuracy(reference_flags, reference_sent_id, candidate_flags, candidate_sent_id)\n",
        "print(\"Accuracy Results:\\n\", accuracy_results)\n",
        "print()\n",
        "\n",
        "# NLG Eval for corrections\n",
        "references, predictions, counters = get_nlg_eval_data(reference_corrections, candidate_corrections)\n",
        "metrics = NLGMetrics()\n",
        "nlg_eval_results = metrics.compute(references, predictions, counters) \n",
        "\n",
        "\n",
        "print(\"NLG Eval Results:\\n\", nlg_eval_results) \n",
        "print()\n",
        "\n",
        "# debug check\n",
        "print(counters)\n",
        "print()\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
